10 差分隐私与机制设计

​	博弈论最吸引人的领域之一是机制设计，这是一门设计激励机制让人们做你想让他们做的事情的科学。事实证明，差异隐私与机制设计有着有趣的联系，这种联系是以两种意想不到的方式出现的。它提供了一个量化和控制隐私损失的工具，如果机制设计者试图操纵的人关心隐私，这一点很重要。然而，它也提供了一种方法来限制机制的结果对任何一个人的选择的敏感性，即使在没有隐私问题的情况下，这也是一种强大的工具。在这一节中，我们对其中的一些想法进行了简要的概述。

​	当算法的输入由个人的、自私的代理而不是算法设计者自己控制时，机制设计就是算法设计的问题。该算法将其报告的输入映射到代理有偏好的某个结果。困难在于代理可能会错误地报告他们的数据，如果这样做将导致算法输出不同的、优选的结果，因此机制设计者必须设计算法，使得代理总是被激励来报告他们的真实数据。

​	机制设计的关注点与私有算法设计的关注点非常相似。在这两种情况下，算法的输入都被认为属于对结果有偏好的某个第三方。在机制设计中，我们通常认为个人从机制的结果中获得一些明确的价值。在私有算法设计中，我们通常认为个体正在经历来自机制结果的一些明显的伤害。事实上，我们可以给出差分隐私的效用理论定义，它等同于标准定义，但与单个效用的联系是明确的:

定义10.1 一个算法$A :$$N^|$$^X$$^|$$→ R$是$\epsilon-$差分隐私,如果对任一函数$f : R → R_+$,且对每一对相邻数据集$x,y ∈ N$$^|$$^X$$^|$:
$$
exp(−ϵ)E_z∼A(y)[f(z)] ≤ E_z∼A(x)[f(z)] ≤ exp(ϵ)E_z∼A(y)[f(z)]
$$
​	对这些结果而言，我们可以将$f$看作是将结果映射到任意代理效用的函数。根据这种解释，一个机制是$ϵ$-差分隐私，在超过$exp(ϵ)$的一个因素下不论他们的效用函数是什么，对每一个代理人来说，它承诺他们对机制的参与不会影响他们预期的未来效用。

​	让我们简单定义一下机制设计中的一个问题。一个机制设计问题由集个对象定义。有$n$个代理$i ∈ [n]$,一系列结果$O$.每一代理有一类型，$ t_i∈ T$只有她自己知道，且对结果有一个效用函数$u : T × O → [0,1]$。代理$i$从结果$ o ∈ O$中获得的效用是$u(t_i, o)$，我们通常将其缩写为$u_i(o)$。我们将$ t ∈ T^n$来表示所有$n$个代理的类型向量，用$ t_i$表示代理$i$的类型，且用$t_-$$_i$$≡ (t_1, . . . $$, t_i$$_-$$_1$, $t_i$ $_+$ $_1$,$$ . . . , t_n)$$表示除了代理$i$外的所有代理的类型向量。代理人$i$的类型完全制定了其结果的效用——也就是说，两个代理人$i\neq j $使得$t_i=t_j$将会同等的评估每一输出：对所有$ o ∈ O \ \   u_i(o) = u_j(o)$.

​	一个机制$M$接受一组报告的类型作为输入，每个玩家一个，并选择一个结果。也就是说，一种机制是映射$$M : T_n→  O$$。代理将选择战略性地报告它们的类型，以便优化它们的效用，可能考虑到(他们认为)其他代理将做什么。特别是，他们不需要向机制报告他们的真实类型。如果一个代理总是被激励去报告某种类型，不管她的对手在报告什么，那么报告那种类型被称为主导策略。如果对每个代理人报告一个人的真实类型是主导策略，那么这种机制被称为真实，或等效地，主导策略真实。

定义10.2 给定一个机制$M : T^n→ O$,真实报告对玩家$i$是$\epsilon$-近似主导策略,如果对每一对类型$t_i,t'_i∈ T$,及对每一个类型向量$t_-$$_i$:
$$
u(t_i, M(t_i, t_−\ _i)) ≥ u(t_i, M(t′_
i, t_−\ _i)) − ϵ.
$$
​	如果真是报道对每个玩家是$\epsilon-$近似主导策略，我们说$M$是$\epsilon-$近似主导策略真实。如果$\epsilon=0$,则$M$是完全真实。

​	也就是说，不论其他玩家报告什么，如果没有代理能通过歪曲她的类型来提高她的效用，则改机制是真实的。

​	在这里，我们可以立即观察到差分隐私定义的句法联系。我们可以用数据空间$X$来标识类型空间$T$。因此，该机制的输入由大小为$n$的数据库组成，该数据库由每个代理的报告组成。事实上，当一个代理考虑她是否应该如实报告她的类型$t_i$或者撒谎，并且错误地报告她的类型为$t'_i$时，她正在决定该机制应该接收两个数据库中的哪一个:$$(t_1，...，t_n)$$，或$t_1，...，t_i\ _- \ _1，t'_i，t_i\ _+\ _1，...，t_n)$。请注意，这两个数据库仅在代理I的报告中有所不同！也就是说，它们是相邻的数据库。因此，差分隐私提供了近似真实性的保证！



10.1 差分隐私作为解决方案概念

研究差分隐私和博弈论之间的联系的出发点之一是观察到差分隐私是比近似更强的条件。注意对$ϵ ≤ 1, exp(ϵ) ≤ 1 + 2ϵ$,以下命题是直接成立的。

​	命题 10.1 如果机制$M$是$\epsilon-$差分隐私，则$M$也是$2\epsilon-$近似主导策略真实。

作为一个解决方案概念，它有几个策略证明机制没有的健壮性特性。根据差分隐私组合原理，两个$\epsilon-$差分隐私机制的构成仍然是$4\epsilon-$近似策略真实。相比之下，一般策略证明机制的激励属性在组合下可能无法保留。

​	作为一个解决方案概念，差分隐私的另一个有用的特性是他推广到组隐私：假设$t,t'∈T^n$不相邻，而是在$k$上不同。回想一下，根据群隐私，我们对任何玩家$i$:$E_o\ _∼\ _M\ _(\ _t\ _)[u_i(o)] ≤exp(kϵ)E_o\ _∼\ _M\ _(\ _t′\ _)[u_i(o)]$.也就是说，最多$k$种类型的变化最多改变了大约$(1+kϵ)$的预期输出，当$k ≪ 1/ϵ$.因此，差分隐私机制使得真实报告成为$2kϵ-$$近似主导策略，即使对于k个代理的联盟也是如此，即不同的隐私自动提供了对共谋的鲁棒性。同样，这与一般的主导策略真实机制形成对比，后者一般不提供防止共谋的保证。

​	值得注意的是，差别隐私允许在非常一般的情况下使用这些属性，而不需要使用金钱！相比之下，在不允许货币转移的情况下，一套确切的主导战略真实机制极其有限。

​	我们的结论是使用差异隐私作为解决方案概念的一个缺点，如前所述:不仅如实报告自己的类型是一种近似的主导策略，任何报告都是一种近似的主导策略！也就是说，不同的隐私使得结果几乎独立于任何单个代理的报告。在某些设置中，这个缺点可以得到缓解。例如，假设$M$是一个差分隐私机制，但是代理效用函数被定义为该机制的结果和代理的报告类型$t'$的函数:形式上，我们将结果空间视为$$O’=  O×T$$。当代理向该机制报告类型$t'$时，并且该机制选择结果$$o ∈  O$$，那么代理所经历的效用由结果$$O′=(O，t′_i)$$控制。现在考虑底层效用函数$$u:T×O′→[0，1]$$。假设我们有一个固定的机制选择$o$，真实的报告是一个主导策略——也就是说，对于所有类型的$$t_i，t’_i$$和所有结果$$o  ∈ O$$:
$$
u(t_i,(o, t_i)) ≥ u(t_i,(o, t′_
i)).
$$
那么事实仍然是，真实地向$\epsilon-$差分隐私机制$$M : Tn→ O$$报告仍然是$$2ϵ$$的近似主导策略，因为对于玩家$i$可能考虑的任何错误报告$t'_i$，我们有:
$$
u(t_i,(M(t), t_i)) = E_o\ _∼\ _M\ _(\ _t\ _)[u(t_i,(o, t_i))]\\
≥ (1 + 2ϵ)E_o\ _∼\ _M\ _(\ _t\ '_i,t−i)[u(t_i,(o, t_i))]\\
≥ E_o\ _∼\ _M\ _(\ _t′_i,\ _t\ _−\ _i)[u(t_i,(o, t′_i))]\\
= u(t_i,(M(t′_i, t_−\ _i), t′_i)).
$$
​	然而，我们不再认为每个报告都是近似的主导策略，因为玩家$i'$的效用可以任意依赖于$$o′=(o，t′_i)$$，并且只有$o$(而不是玩家$i$自身的报告$t'_i$)是差分隐私。我们在这里考虑的所有例子都会是这种情况。

10.2 差分隐私作为机制设计工具

在这一节中，我们展示了差分隐私机制如何被用作设计新机制的工具。

10.2.1 热身：数字商品拍卖

热下身，我们来考虑一个简单的特例，差分隐私在机制设计中的首次应用。考虑一个数字商品拍卖，也就是说，卖家有无限量的商品供应，并且生产的边际成本为零，例如一个软件或其他数字媒体。这种商品有$n$个单位需求买家，每个买家的估价都未知$v_i∈ [0,1]$。非正式地，投标人$i$的估价$v_i$代表买方$i$愿意为商品付出的最大金额。投标人估价没有先验分布，因此自然收入基准是最佳固定价格的收入。价格为$p ∈ [0,1]$,当$v_i≥ p$时每个投标人$i$将购买。因此拍卖人的总收入为
$$
Rev(p, v) = p · |{i : v_i≥ p}|.
$$
最优收益是最优固定价格的收益：$OPT =max_pRev(p, v)$.这种情况已经被很好地研究过了:确切地说，占主导地位的策略真实机制的最佳结果是一种至少实现$ OPT − O(√n)$收入的机制。

我们展示了指数机制的简单应用如何至少获得$OPT − O
(logn/ϵ)$的收益。也就是说，这种机制用精确来换取近似的真实性，但实现了指数级的更好的收入保证。当然，它也继承了前面讨论过的差分隐私的好处，例如对共谋的弹性和可组合性。

这个想法是通过指数机制选择一个价格，使用这个价格将获得的收入作为我们的“质量分数”。假设我们选择指数机制的范围为$R = \{α,2α, . . . ,1\}$.范围大小为$ |R| = 1/α$.如果我们局限于从R中选择一个价格，我们在潜在收入上损失了什么？不难看出
$$
OPT_R≡ max
Rev(p, v) ≥ OPT − αn.
\\ p∈R
$$
这是因为如果$p^*$是实现最优收入的价格，并且我们使用这个价格$p$使得$ p^∗− α ≤ p ≤ p^∗$,每一个以最优价格购买的买家都会继续购买，并为我们每个买家提供最多$\alpha$最少的收入。由于最多有$n$个买家，总损失收入最多为$\alpha n$.

那么我们如何参数化指数机制呢？我们有一族离散范围$R$,用$\alpha$参数化。对一个向量值$v$和一个价格$p ∈ R$,我们定义我们的质量函数为$q(v,p)=Rev(v,p)$.注意因为每个值$v_i∈ [0,1]$,我们可以限制对价格$ p ≤ 1$的关注，因此$q$的敏感度 $∆ = 1:$在至多$ vi≤ 1$下改变一个投标人的估值最多只能改变固定价格下的收入。因此如果我们需要$\epsilon$-差分隐私，通过定理3.11，我们有高概率得到，指数机制返回一些价格$p$使得
$$
Rev(p, v) ≥ (OPT − αn) − O(\frac{1}{\epsilon}ln(\frac{1}α))
.
$$
选择我们的离散化参数$\alpha$来最小化两个误差源，我们发现这个机制很有可能为我们找到一个实现收益的价格
$$
Rev(p, v) ≥ OPT − O(\frac{logn}ϵ).
$$
隐私参数$\epsilon$的正确水平是什么？这里注意，我们不一定将隐私本身视作我们的计算目标。相反，$\epsilon$是一种用代理偏离激励上限来换取收入保证的范式。在经济学中关于大市场的文献中，当确切的真实性遥不可及时，一个共同的目标是“渐进真实性”-也就是说，随着市场规模$n$的增长，任何代理人不得不偏离其真实报告的最大激励趋向于0.为了达到这样的结果，我们需要做的是将$\epsilon$设置为代理人数量$n$的递减函数。例如，如果我们取$\epsilon = 1/log(n)$,那么我们获得了一个渐进完全真实的机制（即，随着市场的增长，真实性的近似值变得精确）。我们也可以问，醉着$n$的增加，我们对最佳收入的近似值是多少。注意我们对最佳收入的近似只是累加的，因此即使这样设定$\epsilon$，我们仍能保证收入至少为$(1-o(1))OPT$,只要在人数规模为$n$时$OPT$的增长速度比$log(n)^2$增长更快。

最后，注意，我们可以将每个代理人$i$和报告值$v_i$绑定。换句话说，我们可以将一个项目分配给代理$i$,并在$vi≥ p$时提取所选已过账价格$p$的付款。如果我们这样做，这个机制是近似真实的，因为价格是使用不同的私有机制选择的。此外，并不是每份报告都是一种近似的主导策略:如果代理人超额报告，她可能被迫以高于其真实价值的价格购买商品。

10.2.2 近似真实均衡选择机制

我们现在考虑近似真实均衡选择的问题，我们回想 纳什均衡的定义：假设每个玩家有一组动作$A$,并且可以选择做任何动作$ ai∈ A$.此外，假设结果仅仅是代理人可能选择做的动作的选择，因此代理人效用函数被定义为$ u : T × A^n→ [0,1].$则：

定义10.3 一组动作$ a ∈ A^n$是$\epsilon$近似纳什均衡，如果对所有参与者$i$和所有动作$a'_i$:
$$
u_i(a) ≥ u_i(a′_i, a_−\ _i) − ϵ
$$
换句话说，假设他们按照$a$进行动作，每个代理同时对其他代理正在做的事情做出（近似的）最佳相应。

大致来说，问题如下:假设给我们一个游戏，其中每个玩家都知道自己的收益，但不知道别人的收益(即玩家不知道其他代理的类型)。因此，玩家不知道这个游戏的平衡结构。即使他们做到了，也可能存在多重平衡，不同的代理人更喜欢不同的平衡。由中介提供的机制能激励代理人如实报告他们的效用并遵循它选择的均衡吗？

例如，想象一个城市，其中(比方说)谷歌导航是主导服务。每天早上，每个人输入他们的起点和目的地，接收一组方向，并根据这些方向选择他/她的路线。有没有可能设计一个导航服务，使得:每个代理都被激励(1)如实报告，然后(2)遵循提供的驾驶方向？错误地报告起点和终点，以及真实地报告起点和终点，但随后遵循不同的(较短的)路径都是不鼓励的。

直觉上，我们两个的渴望是冲突的。在上面的通勤例子中，如果我们要保证每个玩家都被激励真实地遵循他们建议的路线，那么我们必须根据玩家的报告计算出所讨论的游戏的均衡。另一方面，要做到这一点，我们给某个玩家$i$的建议路线必须依赖于其他玩家报告的位置/目的地对。这种紧张关系会在激励方面造成问题:如果我们在给定玩家报告的情况下计算游戏的均衡，代理人可能会因误报而受益，导致我们计算错误游戏的均衡。

然而，如果$i$号代理人的报告对$j\neq i$代理人的行动只有很小的影响，这个问题就会得到很大的缓解。在这种情况下，代理人$i$很难通过他对其他玩家的影响获得优势。然后，假设每个人都如实报告了他们的类型，该机制将计算出正确游戏的均衡，根据定义，每个代理人$i$只能按照建议的均衡行动去做。换句话说，如果我们可以在差分隐私的约束下计算出游戏的近似均衡，那么真实的报告，然后采取协调设备的建议动作，将是纳什均衡。稍加思考就会发现，私下计算均衡的目标在小游戏中是不可能的，在小游戏中，一个代理人的效用是其他代理人的行为(因此也是效用函数)的高度敏感函数。但是大型游戏呢？

形式上，假设我们有一个动作集为$A$的$n$人游戏，每一个类型为$t_i$的代理都有一个效用函数$u_i: A^n→ [0,1].$我们说这个游戏是$\Delta-large$如果对所有玩家$i\neq j$,动作向量$a\in A^n$,且动作对$a_j,a'_j \in A$:
$$
|u_i(a_j, a_−\ _j) − u_i(a′_j, a_−\ _j)|≤ ∆
$$
换句话说，如果某个代理人$j$单方面改变了他的行为，那么他对任何其他代理人$i\neq j$的收益的影响最多是$\Delta$。注意，如果代理$j$改变了他自己的行为，那么他的收益可以任意改变。很多游戏都是这个意义上的“大”。在上面的通勤例子中，如果爱丽丝改变她的工作路线，她可能会大大增加或减少她的通勤时间，但对任何其他代理鲍勃的通勤时间的影响很小。本节中的结果对$\Delta = O(1/n)$最强，但更普遍。

首先，我们可能会问我们是否需要隐私——在一个大型游戏中，任何由类型报告定义计算的的游戏的平衡的算法是否具有我们想要的稳定性？答案是否定的，举个简单的例子，考虑$n$个人，他们必须各自选择是去海滩(B)还是去山里(M)。人们私下里知道他们的类型——每个人的效用取决于他自己的类型、他的行为和去海滩的其他人$p$的比例。海滩型的人如果去海滩会得到$10p$的报酬，如果去山上会得到$5p$的报酬。一个山地类型从参观海滩中获得$5p$的回报，从参观山地中获得$10(1-p)$的回报。请注意，这是一个大型(即低敏感度)游戏——每个玩家的收益对其他人的行为不敏感。此外，请注意，“每个人都去海滩”和“每个人都去山”都是游戏的均衡，不管类型的实现如何。考虑试图实现以下社会选择规则的机制——“如果海滩类型的数量少于人口的一半，就把每个人都送到海滩，反之亦然。”应该清楚的是，如果山地类型只是大多数，那么每一种山地类型都有被误报为海滩类型的动机；反之亦然。因此，即使游戏是“大”的，并且代理人的行为不会显著影响其他人的收益，简单地根据报告的类型配置文件计算均衡通常也不会导致近似真实的机制。

然而，它被证明有可能给出一个具有以下性质的机制:它引出每个代理$t_i$的类型，然后计算由报告的类型定义的博弈的$\alpha-$近似相关均衡。(在某些情况下，有可能加强这个结果来计算基础博弈的近似纳什均衡。)它从相关均衡中画出一个行动剖面$a\in A^n$，并向每个代理$i$报告行动$a_i$。该算法保证同时所有玩家$i$，报告的联合分布$a_-\ _i$对所有玩家除了$i$在代理$i$的报告的类型中是差分隐私的。当该算法计算底层游戏的相关均衡时，  这种保证对于近似真实的受限形式来说是足够的:可以选择加入或退出该机制的代理人(但如果他们选择加入，就不会错误地报告他们的类型)没有选择退出的抑制因素，因为没有代理人$i$可以通过选择退出来实质性地改变对其他玩家的行为的分布。  此外，如果他选择加入，没有代理人有激励不遵循他建议的行动，因为他的建议是相关均衡的一部分。当该机制计算出底层博弈的纳什均衡时，即使代理人在选择加入时有能力向该机制误报他们的类型，该机制也会变得真实。

更具体地说，当这些机制在满足$\epsilon-$差分隐私的同时计算$\alpha$近似纳什均衡时，每个遵循诚实行为(即，首先选择并报告他们的真实类型，然后遵循被建议的行为)的代理形成$$(2ϵ  +  α)$$近似纳什均衡。这是因为，根据隐私，报告你的真实类型是$2\epsilon$近似主导策略，并且假设每个人都报告他们的真实类型，该机制计算真实游戏的$\alpha$近似均衡，因此根据定义，遵循建议的动作是$\alpha$近似最佳响应。在$ \alpha=O(\frac{1}{\sqrt n \epsilon})$的大型博弈中，存在计算和α近似均衡的机制。因此，通过设置$\epsilon = O(\frac{1}{n^1\ ^/\ ^4})$，这给出了$\eta-$近似真实的均衡选择机制:
$$
η = 2ϵ + α = O(\frac{1}{n^1\ ^/\ ^4})
.
$$
换句话说，它给出了一种在大型游戏中协调均衡行为的机制，这种机制在游戏规模上是渐近真实的，不需要货币转移。

10.2.3 获得确切的真实性

到目前我们已经讨论了在大群体博弈中渐进真实的机制。然而，如果我们想坚持那些真正具有主导战略真实的机制，同时保持我们机制到目前所享有的一些优良特性：例如，这些机制不需要能够提取货币支付，那该怎么办？差分隐私在这里有帮助吗？它可以-在这一节中，我们讨论一个框架，它使用差分隐私机制作为模块来设计真正的无金钱真实机制。

基本思路是简单优雅。正如我们所看到的，指数机制通常可以在保持差异隐私的同时提供出色的效用保证。这并没有产生一个完全真实的机制，但它给了每个代理人很少的动机去偏离真实的行为。如果我们能把它与第二种机制结合起来呢？第二种机制不需要很好的效用保证，但是给每个代理人一个严格的积极激励去如实报告，也就是说，这种机制本质上只惩罚不真实的行为。然后，我们可以在运行这两种机制之间随机选择。如果我们对惩罚机制给予足够的重视，那么我们就继承了它的严格真实性。放在指数机制上的剩余权重有助于最终机制的效用属性。希望由于指数机制从一开始就是近似的策略证明，随机机制可以对严格真实的惩罚机制施加较小的权重，因此将具有良好的效用特性。

为了设计惩罚机制，我们将不得不在稍微不标准的环境中工作。不可简单地选择一个结果，我们可以将一个机制建模为选择一个结果，然后将一个代理建模为选择对该结果的反应，这两者共同定义了他的效用。然后，机制将有权根据代理所报告的类型来限制代理所允许的反应。在形式上，我们将在以下框架内工作:

定义10.4 （环境）一个环境是是$n$个参与者的集合$N$,一组类型$t_i\in T$,一组有限的结果$Q$,一组反应$R$,和一个效用函数$u:T\times O\times T\rightarrow [0,1]$.

在选择$\hat R_i\subseteq R$中我们写$r_i(t,s,\hat R_i)\in arg max_r\ _\in\ _\hat R\ _i u_i(t,s,r)$来表示最优反应，如果对可选择的$s$他是类型$t$.

一个直接揭示机制$M$定义了如下玩法的游戏：

​	1.每个玩家$i$报告一个类型$t'_i\in T$.

​	2.该机制为每个玩家$i$选择一个备选$s\in O$和一个反应子集$\hat R_i\subseteq R$.

​	3.每个玩家$i$选择一个反应$r_i\in \hat R_i$,和经验效用$u(t_i,s,r_i)$.

代理人玩游戏是为了最大化自己的效用。注意由于在第三步后没有进一步的交互，理性的代理人回选择$r_i=r_i(t_i,s,\hat R_i)$,因此我们可以忽视这作为策略步骤。令$R=2^R$,则该机制就是以随机映射$M:T\rightarrow O\times R^n$.

让我们考虑实用福利标准：$F(t,s,r)=\frac{1}{n}\sum_{i=1}^nu(t_i,s,r_i)$,注意这里有敏感度$\Delta=1/n$,因为每个代理的效用都在$[0,1]$范围内。因此，如果我们简单选择一个结果$s$,且允许每个代理人做出他们最佳反应，指数机制是一个$\epsilon$-差分隐私机制，根据定理3.11，以高概率实现社会福利至少为$OPT-O(\frac{log|O|}{\epsilon n})$.让我们用质量分数$F$,范围$O$,隐私参数$\epsilon$来表示指数机制的这个实例，为$M_\epsilon$.

这个想法是在指数机制（具有良好的社会福利属性）和惩罚虚假报告（但具有较差的社会福利属性）的严格真实机制之间随机化。如果我们适当的混合，那么我们将得到一个具有合理社会福利保障的完全真实的机制。

这里有一个这样的惩罚机制，简单但对于给定的问题不一定是最好的：

定义10.5：承诺机制$M^P(t')$随机均匀选择$s \in O$,并设置$\hat R_i = r_i(t'_i,s,R_i)$,即他选择一个随机结果并强迫每个人做出反应，就像他们报告的类型是他们的真实类型。

将环境的差距定义为：
$$
γ = \smash{\displaystyle\min_{i,ti=t′i,t_{-i}}} \smash{\displaystyle\max_{s∈O}}(u(t_i, s, r_i(t_i, s, R_i)) - u(t_i, s, r_i(t′_i, s, R_{i}))) ,
$$
即，$\gamma$是参与者的下限，是是错误报告的最坏成本(超过$s$)的类型。注意，对于每个玩家，这种最坏情况的发生概率至少为$1/|O|$.因此，我们有以下观察：

引理10.2 对于所有$i,t_i,t'_i,t_{-i}$:
$$
u(t_i,M^P(t_i,t_{-i}))\geq u(t_i,M^p(t'_i,t_{-i}))+\frac{\gamma}{|O|}
$$
​	注意承诺机制是严格真实的：每个人至少有一个$\frac{\gamma}{|O|}$激励他们不去撒谎。

​	这表明了一个具有良好社会福利保证的完全真实的机制：

定义10.6 用参数$0\leq q\leq 1$定义的乘法指数机制$M_\epsilon ^P(t)$选择概率为$1-q$的指数机制$M_\epsilon(t)$,和互补概率为$q$的惩罚机制$M^P(t)$.

​	通过对期望线性的观察，对所有$t_i,t'_i,t_{-i}$:
$$
u(t_i,M_\epsilon^P(t_i,t_{-i}))=(1-q)\cdot u(t_i,M_\epsilon(t_i,t_{-i}))+q\cdot u(t_i,M^P(t_i,t_{-i}))\\ \geq(1-q)(u(t_i,M_\epsilon(t'_i,t_{-i}))-2\epsilon)\\ +q(u(t_i,M^P(t'_i,t_{-i}))+\frac{\gamma}{|O|})\\ =u(t_i,M_\epsilon^P(t_i',t_{-i}))-(1-q)2\epsilon+q\frac{\gamma}{|O|}\\ =u(t_i,M^P_\epsilon(t'_i,t_{-i}))-2\epsilon+q(2\epsilon+\frac{\gamma}{|O|})
$$
​	以下两个定理表明了该机制的激励和社会福利性质。

定理10.3 若$2\epsilon\leq \frac{q\gamma}{|O|}$则$M_\epsilon^P$是绝对真实的。

​	注意，我们对此机制也有实用程序保证。设置参数$q$使得我们有一个真实的机制：
$$
\begin{align}
E_{s,\hat R∼M^P_ϵ}& [F(t, s, r(t, s, \hat R))]\\
&≥ (1 - q) · E_{s,\hat R∼M_ϵ }[F(t, s, r(t, s,\hat R))]\\
&= (1 -\frac {2ϵ|O|}γ ) · E_{s,\hat R∼M_ϵ} [F(t, s, r(t, s,\hat R))]\\
&≥ (1 -\frac{2ϵ|O|}γ ) · (\smash{\displaystyle\max_{t,s,r}}F(t, s, r) - O ( \frac{1}{ϵn}log |O|))\\
&≥ \smash{\displaystyle\max_{t,s,r}}F(t, s, r) -\frac{ 2ϵ|O|}{γ} - O (\frac{1}{ϵn}log |O|) .
\end{align}
$$
设置
$$
\epsilon \in O(\sqrt\frac{log|O|\gamma}{|O|n})
$$
我们发现：
$$
E_{s,\hat R~M_\epsilon^P}[F(t,s,r(t,s,\hat R))]\geq \smash{\displaystyle\max_{t,s,r}}-O(\sqrt\frac{|O|log|O|}{\gamma n}).
$$
注意，在此计算中，我们假设$\epsilon\leq \gamma/(2|O|)$使得$q=\frac{2\epsilon|O|}{\gamma}\leq 1$且机制定义明确。对于足够大的$n$这是正确的。即，我们有展示：

定理10.4 对于足够大的$n,M_\epsilon^P$至少获得社会福利
$$
OPT-O(\sqrt\frac{|O|log|O|}{\gamma n}).
$$
​	注意，此机制是真实的无需付款！

现在让我们考虑一下这种框架的应用：便利性位置游戏。假设一个城市想要建立$k$家医院以使每个公民与其最近的医院间的平均距离最小。为了简化问题，我们温和假设城市建立在单位线的离散化基础上。正式地，使$L(m) = {1,\frac{1}m,\frac{2}m,...1}$代表步长为$1/m$的离散单位线。$|L(m)|=m+1$.对所有$i$令$T=R_i=L(m)$且令$|O|=L(m)^k$.

将代理$i$的效用定义为
$$
u(t_i,s,r_i)=\begin{cases}
-|t_i-r_i|,&If r_i\in s;\\
-1,& otherwise
\end{cases}
$$
换句话说，代理与线上的点相关联，结果是将线上的位置分配给$k$个设施中的每一个。代理可以通过决定去那一组设施来做出反应，而决策成本是他们自己的位置（即他们的类型）与他们选择的设施之间的距离。注意$r_i(t_i,s)$在这里是最接近的设施$r_i\in s$.

我们可以实例化定理10.4。在这种情况下，我们有:$|O|=(m+1)^k$和$\gamma = 1/m$,因为任何两个位置$t_i\neq t_j$相差至少$1/m$.因此，我们有：

定理10.5.为设施定位游戏实例化的$M_\epsilon^P$是严格真实的，并且至少实现的社会福利至少为：
$$
OPT-O(\sqrt\frac{km(m+1)^klogm}{n}).
$$
这对于少量的的设施$k$已非常好，因为我们期望$OPT=\Omega(1)$. 

10.3隐私感知代理的机制设计

在上一节中，对于只关心机制选择结果的代理，我们看到差分隐私可以用作设计机制的工具。这里我们主要将隐私视为在传统机制设计中实现目标的工具。副作用是，这些机制还保留了所报告玩家类型的隐私。这本身是一个值得的目标吗？为什么我们想要我们的机制来保护代理类型的隐私？

一些反思表明代理人可能关心隐私。确实，基本的内省表们，在现实世界中，代理人重视将某些“敏感”信息保密的能力，例如，健康信息或性别偏爱。在本节中，我们考虑如何这类隐私值建模的问题以及文献中采用的各种方法。

考虑到代理人或许会对隐私有偏爱，因此值得考虑将保护隐私作为机制设计的额外目标。即使对于那些我们已经可以私下解决的福利最大化之类的任务。就像我们所看到的，确实有可能将VGG机制推广到任意社会选择问题中以私下近似优化社会福利，在隐私参数和近似参数之间进行平滑的权衡，同时保证确切的主导策略真实性。

然而，我们可能希望走得更远。在具有隐私偏好的代理存在的情况下，如果我们希望设计真实的机制，我们必须以某种方式在其效用函数中对他们的隐私偏好建模，然后设计关于这些新的“隐私感知”效用函数的真实机制。正如我们在差分隐私中看到的，将隐私建模为机制本身的属性是最自然的。因此我们的效用函数不仅仅是结果的函数，而是结果和机制本身的函数。在几乎所有的模型中，用于结果的代理效用被视为线性可分的，即，我们对每个代理$i$有：
$$
u_i(o,M,t)\equiv \mu_i(o)-c_i(o,M,t).
$$
这里$\mu(o)$表示代理$i$对结果$o$的效用，$c_i(o,M,t)$表示代理$i$在机制$M$选择结果$o$时所经历的（隐私）成本。

我们将首先考虑隐私代价函数$c_i$的最简单（也是最幼稚）的模型。回想一下对于$\epsilon\ll 1$,差分隐私承诺对每个代理$i$和每个可能的效用函数$f_i$,类型向量$t\in T^n$,和偏差$t'_i\in T$:
$$
|E_{o∼M(t_i,t_{−i})}[f_i(o)] − E_{o∼M(t′_i,t_{−i})}[f_i(o)]| ≤ 2ϵE_{o∼M(t)}[f_i(o)]
$$
如果我们认为$f_i$代表了代理$i$的”期望未来效用“，那么很自然地要对代理$i$的成本建模，在$\epsilon$-差分隐私计算中使用其数据在$\epsilon$是线性的。即，我们认为代理$i$是由某些值$v_i\in R$参数化的，取：
$$
c_i(o,M,t)=\epsilon v_i,
$$
其中$\epsilon $是使得$M$满足$\epsilon $-差分隐私的最小值。这里我们设想$v_i$代表一个像$E_{o~m(t)}[f_i(o)]$这样的量。在这种设置下，$c_i$不依赖于结果$o$或是类型描述$t$.

使用这种天真的隐私措施，我们讨论了隐私数据分析中的一个基本问题:当数据所有者重视他们的隐私并坚持为此获得补偿时，如何收集数据。在这种情况下，除了支付外没有代理重视的“结果”，只有隐私损失的无效用。然后，我们将讨论这种(和其他)隐私损失无效性度量的缺点，以及当代理对机制的结果有效时，在更一般的机制设计设置中的隐私。

10.3.1 VCG机制的隐私概括

假设我们有一个一般的社会选择问题，由一个结果空间$O$定义，一组对给出结果有任意偏好$u_i:O\rightarrow[0,1]$的代理$N$.我们可能想选择一个输出$o\in O$来最大化社会福利$F(o)=\frac{1}{n}\sum_{i=1}^n u_i(o)$.众所周知，在任何这种设定下，VCG机制都可以实现社会福利最大化的结果$o^*$,同时收取费用使讲真话成为主导策略。如何在保护隐私的同时我们能实现同样的结果呢？该如何权衡隐私参数$\epsilon $和我们对最佳社会福利的近似值？

回想一下，我们可以通过质量分数$F$来使用指数机制选择结果$o\in O$。对于隐私参数ϵ，这将给出分布$M_\epsilon$定义为$Pr[M_\epsilon=o]∝exp(\frac{\epsilon F(o)}{2n})$。此外，该机制具有良好的社会福利性质:以$1-\beta$的概率，它选择一些o，使得:$F(o)\geq F(o^*)-\frac{2}{\epsilon n} (ln\frac{|O|} {\beta})$。但正如我们所见，差分隐私只给出$\epsilon$-近似真实。

然而，可以看出$M_\epsilon $是下面精确优化问题的解：
$$
M_\epsilon = arg\ \smash{\displaystyle\max_{D\in \Delta O}}(E_{o~D}[F(o)]+\frac{2}{\epsilon n}H(D)),
$$
其中$H$代表分布$D$的香农熵。换句话说，指数机制是使期望社会福利最大化的分布，加上由$\frac{2}{\epsilon n}$加权的分布熵。这是有意义的，理由如下：众所周知，在任何有限范围内任何精确最大化期望玩家效用的机制（在分布范围机制中被称为最大）可以与支付配对，以使精确的主导策略真实。指数机制是精确的使期望社会福利最大化的分布，加上熵。换句话说，如果我们增加了一个额外玩家，其效用正好是分布的熵，则指数分布在分布范围内是最大的。因此，它可以与支付结合，使真实报告成为所有玩家的主导策略-特别是对$n$个真实玩家。此外，它可以展示如何以保护隐私的方式来收取费用。结果是，对任何社会选择问题，社会福利都可以以一中既保护不同隐私，又完全真实的方式来近似。

10.3.2 敏感检验员的问题

在这节，我们靠以一个数据分析师的问题，他希望使用一组个人的隐私数据进行研究。然而，他必须说服这些人去交出他们的数据！个人付出隐私损失的代价。数据分析师可以通过保证差分隐私和赔偿他们的损失来减少这些代价，同时试图得到有代表性的数据样本。

考虑一下敏感的检验员爱丽丝的以下格式化问题。他的任务是对N中$n$个个体进行调查，以确定满足某些属性$P(i)$的个体$i\in N$的比例是多少。她的最终目标是发现这个统计量的真实值，$s=\frac{1}{n}|i\in N:P(i)|$,但如果那不可能，她会对一些使得误差$|\hat s -s|$最小的估计值$\hat s$满意。我们将采用一个基于大偏差范围的精确度概念,并说如果$Pr[|\hat s-s|]\leq \frac{1}{3}$则测量机制是$\alpha$-准确。不可避免的是，每个人都很重视自己的隐私，不会免费参与调查。在与爱丽丝互动时，个人会因为失去隐私而付出一些代价，必须为此得到补偿。更糟糕的是，这些人是理性的(也就是说，自私的)代理人，如果这样做会带来经济利益，他们就倾向于向爱丽丝谎报他们的成本。这将Alice的问题直接置于机制设计的领域，并要求Alice开发一种方案，以平衡统计准确性与成本，同时管理个人的激励。

顺便说一句，这个程式化的问题与任何使用潜在敏感数据集合的组织都广泛相关。这包括,例如,使用搜索日志提供完成搜索查询，使用浏览历史提高搜索引擎排名,使用社交网络的数据选择显示广告及推荐新的链接,和无数其它网络上由数据驱动的服务。在所有这些情况下，价值都是从敏感数据收集的统计属性中获得的，以换取一定的报酬。

以固定的价格收集数据可能会导致对人口统计数据的估计有偏差，因为这样的计划只会导致收集数据的人的隐私价值低于提供的价格。但是，在没有与代理商互动的情况下，我们无法知道我们可以提供什么价格，因此我们将有足够广泛的参与，以保证我们收集的答案只有很小的偏差。因此，为了获得对统计数字的准确估计，人们自然会考虑通过拍卖来购买私人数据——作为发现价格的一种手段。在进行私人数据的拍卖时，必须面对两个明显的障碍，另外一个障碍不那么明显，但更加阴险。第一个障碍是，必须对“隐私”有一个定量的形式化描述，可以用来衡量在其数据的各种操作下的代理的成本。在这里，差分隐私提供了一个明显的工具。对于值较小的$\epsilon$,因为$$exp(ϵ)≈(1  +ϵ)$$,正如前面所讨论的,一个简单的(但可能天真的)在一个模型的第一次削减，是把每个参与隐私研究的代理视作具有线性成本。当数据被用在一个$\epsilon$-差分隐私机制中，我们假设每个代理$i$有一个位置的隐私值$v_i$,经验成本$c_i(\epsilon) = \epsilon v_i$。第二个障碍是,我们的目标是用统计准确性来权衡，而后者在机制设计中没有得到充分研究。

最后,更阴险的障碍是，个人隐私损失的成本可能与其个人隐私数据本身高度相关!假设我们只知道Bob对其艾滋病状态的隐私很看重，而不明确地知道他的艾滋病状态本身。这已经是显而易见的了，因为鲍勃的艾滋病状况很可能与他的隐私价值有关，知道他的隐私需要付出很高的代价，让我们更新了对他的隐私数据的看法。更确切地说，假设在艾滋病流行率调查的第一步中，我们要求每个人报告他们的隐私价值，然后进行拍卖，以选择从哪些人那里购买数据。如果代理如实报告，我们会发现报告的价值自然形成了两个集群:低价值代理和高价值代理。在这种情况下，我们可能在收集任何数据或支付任何费用之前就已经了解了一些关于人口统计的信息——因此，代理将已经付出了代价。因此，代理可能会误报他们的价值，这可能会在调查结果中引入偏见。这一现象直接揭示机制存在问题，并将此问题与经典机制设计区分开来。

一个代理$i$的损失带着一种量化手段允许他的数据被$\epsilon$-差分隐私算法$(c_i(\epsilon)=\epsilon v_i)$使用,我们几乎准备好描述面案测量员的问题的结果了。对于一般类型空间$T$,差分隐私算法是某些映射$M:T^n\rightarrow  O$.仍需定义类型空间$T$具体是什么。我们将会考虑两个模型。在两个模型中，我们将每个个体关联一个位$b_i\in \{0,1\}$,表示其是否满足敏感谓词$P(i)$,以及一个隐私值$v_i\in R^+$.

​	1.在不敏感价值模型，令类型空间$T=\{0,1\}$,我们计算隐私机制的$\epsilon$参数：即，我们只根据机制如何对待敏感位$b_i$来衡量隐私成本，而忽略了它如何对待所报道的隐私价值$v_i$.

​	2.在敏感的价值模型，令类型空间$T=(\{0,1\}\times R^+)$,我们计算隐私机制的$\epsilon$参数：即我们根据它如何对待每个个体的对$(b_i,v_i)$.

直觉上，不敏感价值模型认为个体忽略了隐私价值与隐私位之间的相关性所带来的潜在隐私损失，而敏感价值模型则认为个体将这些相关性假设为最坏情况，即个体的价值$v_i$与隐私位$b_i$的价值一样容易泄露。已知，在不敏感值模型中，可以推导出近似最优的直接揭示机制，从而达到高精度和低成本的目的。而在敏感值模型中，没有任何个体理性的直接揭示机制能够达到非平凡的准确性。

这就留下了一种不太令人满意的状态。敏感的价值模型捕捉到了我们真正想要处理的微妙问题，然而我们却有一个不可能的结果！以一种令人满意的方式(例如，通过改变模型或机制的能力)来解决这个问题仍然是一个有趣的未决问题。

10.3.3 隐私代价的更好衡量标准

在前一节中，我们采用了天真的建模假设，即对某些数值值$v_i$参与$\epsilon$-差分隐私机制$M$所付出的代价是$c_i(o，M，t) = ϵv_i$.这措施有几个问题。第一，尽管差分隐私承诺任何代理人的效用损失都是以$\epsilon$的近似线性数量为上界，没有理由认为代理人的待机时以这种量化为下界的。即，尽管取$c_i(o,M,t)\leq\epsilon v_i$是一个很好的动机，但很少人支持将不平等作为平等。第二，（事实证明）任何隐私衡量，只要是$\epsilon$的确定性函数（而不仅仅是线性函数）都会导致有问题的行为预测。

那么我们该怎么样建模$c_i$?一个自然的度量是单立人$i$报告的类型和机制结果之间的相互信息。为了很好的定义，我们必须在一个这样的世界中，每一个代理人的类型$t_i$都是从一个已知先验中获得，$ti∼ T$.每个代理的策略是一个映射$\sigma_i:T\rightarrow T$,根据其真是类型确定其报告的类型。然后我们可以定义：
$$
c_i(o,M, σ) = I(T ;M(t_{−i}, σ(T )),
$$
其中$I$表示代理的类型的先验的随机变量$T$和代表机制结果的随机变量$M(T_{-i},\sigma(T))$之间的相互信息，给定代理策略。

这种方法有很大吸引力，因为它代表了该机制的输出与代理$i$的真实类型是如何”相关“的。然而，除了要求优先于代理类型之外，观察到一个有趣的悖论，它是由这种隐私损失的度量产生的。考虑一个有两种三明治面包的世界:黑麦（R）和小麦(W)。此外，在这个世界上，三明治偏好是非常尴尬和保密的。在类型$T$上的优先权在R和W上是一致的，机制$M$只是给代理$i$一个他声称更喜欢的三明治类型。现在考虑两种可能的策略，$\sigma_{truthful}$和$\sigma_{random}$。$\sigma_{truthful}$对应于如实报告三明治偏好(并随后导致吃偏好的类型的三明治)，而$\sigma_{random}$随机报告独立于真实类型(并导致偏好的三明治只有一半的时间)。使用随机策略的代价是$I(T ;M(t_{−i}, σ_{random}(T )) = 0$,因为输出与代理$i$的类型无关.另一方面，如实报告的代价是$I(T；M(T_{-i}，σ_{true}(T))= 1$，因为三明治结果现在恒等函数在代理是类型。然而，从任何外部观察者的角度来看，这两种策略是无法区分的!在这两种情况下，代理$i$都会收到一个均匀随机的三明治。那么为什么每个人都要选择随机策略呢?只要对手认为他们是在随机选择，他们就应该选择诚实的策略。

另一种方法不需要代理类型的优先级，如下所示。我们可以将代理建模为具有一个成本函数$c_i$满足:
$$
|c_i(o,M,t)|=ln(\smash{\displaystyle\max_{t_i,t'_i\in T}}\frac{Pr[M(t_i,t_{-i}=o)]}{Pr[M(t'_i,t_{-i})=o]}).
$$
注意如果$M$是$\epsilon$差分隐私，则
$$
\smash{\displaystyle\max_{t\in T^n}\max_{o\in O}\max_{t_i,t'_i\in T}}\frac{Pr[M(t_i,t_{-i}=o)]}{Pr[M(t'_i,t_{-i})=o]})\leq \epsilon
$$
也就是说，我们可以把差分隐私看作是所有可能结果中隐私损失最坏情况的边界，而这里提出的措施只考虑实际实现的结果$o$(和类型向量$t$)的隐私损失。因此,对所有$o,t$对于任何差分隐私机制$M,|c_i(o,M, t)|≤ϵ$,但重要的是成本可以通过结果变化。

然后我们可以考虑以下最大化社会福利的分配规则$F(o) =∑^n_{i=1}u_i(o)$。我们讨论当$|O| =  2$(不需要支付)的情况，但它有可能分析一般情况(支付)，它私密实现了任何社会选择问题的VCG机制。

​	1.对于每个结果$o∈o$，从分布$Pr[r_o= x] ∝ exp(−ϵ|x|).$中选择一个随机数$r_o$.

​	2.输出$o^∗= arg\ max_{o∈O}(F(o) + r_o)$.

上述机制是$\epsilon$-差分隐私,并且对于隐私感知代理是真实的,只要每个代理$i$,两种结果$o, o '∈O, |µ_i(o)−µ_i (o) | > 2ϵ$。请注意，这对于足够小的ϵ来说是正确的，只要结果的代理效用是不同的。该分析通过考虑随机变量$r_o$的任意固定实现和代理$i$的真实报告的任意偏差来进行。有两种情况:在第一种情况下，偏差不会改变机制的结果$o$。在这种情况下，代理人对结果$\mu_i$的效用和他的隐私损失代价$c_i$都没有改变，因此代理人不会从偏离中获益。在第二种情况下，如果当代理$i$偏离时，结果从$o$变为$o'$,则必须是$\mu(o ′)< \mu(o )- 2ϵ$.然而，通过差分隐私，$$|c_i(o，M，t)-c_i(o′，M，t)| ≤ 2ϵ$$，因此隐私成本的变化不足以使其受益。

最后，通常认为最保守的隐私成本建模方法如下。给定一个$\epsilon$-差分隐私机制$M$，仅假设
$$
c_i(o,M,t)\leq \epsilon v_i,
$$
对于某些数字$v_i$.这类似于我们之前考虑的线性成本函数，但关键是，这里我们只假设一个上限。到目前为止，我们考虑的所有其他隐私代价模型都满足了这一假设。可以看出，许多机制将一个有差分隐私算法与一个具有限制用户选择能力的惩罚机制相结合，就像我们在第10.2.3节中考虑的那些机制一样，只要值$v_i$是有界的，在有隐私偏好的代理存在的情况下，保持它们的真实性。